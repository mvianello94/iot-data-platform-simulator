services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.1
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true" # Useful for development
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "kafka-topics --bootstrap-server localhost:9092 --list > /dev/null || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  schema-registry:
    image: confluentinc/cp-schema-registry:7.9.1
    container_name: schema-registry
    hostname: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/subjects || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000" # S3 API port
      - "9001:9001" # Console UI port
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ':9001'
    volumes:
      - minio_data:/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:9000/minio/health/live || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 minioadmin minioadmin;
        mc ls local/iot-data >/dev/null 2>&1 || mc mb local/iot-data; # Create bucket if it doesn't exist
        mc policy set public local/iot-data;
      "

  devices-simulator:
    build:
      context: ./devices-simulator
    container_name: devices-simulator
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092

  spark-stream-processor:
    build:
      context: ./spark-stream-processor
    container_name: spark-stream-processor
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      PYSPARK_PYTHON: python3
      SPARK_LOCAL_IP: 0.0.0.0
      ICEBERG_CATALOG: iot_data
    volumes:
      - ./config/spark:/opt/bitnami/spark/conf

  # spark-thrift-server:
  #   image: apache/spark:3.5.6
  #   container_name: spark-thrift-server
  #   depends_on:
  #     - minio
  #   ports:
  #     - '10000:10000'
  #   environment:
  #     # Configure Spark for S3A access to MinIO
  #     SPARK_DAEMON_JAVA_OPTS: "-Dspark.hadoop.fs.s3a.endpoint=http://minio:9000 -Dspark.hadoop.fs.s3a.access.key=minioadmin -Dspark.hadoop.fs.s3a.secret.key=minioadmin -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
  #   command: >
  #     /opt/spark/sbin/start-thriftserver.sh
  #     --master local[*]
  #     --conf spark.sql.warehouse.dir=s3a://iot-data/events
  #     --conf spark.ui.showConsoleProgress=true
  #     --conf spark.eventLog.enabled=true
  #     --conf spark.eventLog.dir=/tmp/spark-events
  #   volumes:
  #     - ./spark-conf:/opt/spark/conf
  #     - ./spark-events:/tmp/spark-events

  superset:
    image: apache/superset:latest
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      SUPERSET_ENV: development
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_SECRET_KEY: "your_super_secret_key_here_change_this_in_production!"
    volumes:
      - superset_home:/app/superset_home
    depends_on:
      minio:
        condition: service_healthy
    command: >
      /bin/sh -c "
        superset db upgrade &&
        superset init &&
        superset fab create-admin --username admin --firstname Admin --lastname Admin --email admin@example.com --password admin123! || true && # Prevent failure if admin already exists
        superset run -p 8088 --host 0.0.0.0
      "
    restart: on-failure

volumes:
  kafka_data:
  zookeeper_data:
  zookeeper_log:
  minio_data:
  superset_home:
