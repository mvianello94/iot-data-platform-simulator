services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.1
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "kafka-topics --bootstrap-server kafka:29092 --list > /dev/null || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    command: >
      "
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic _connect-configs --replication-factor 1 --partitions 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic _connect-offsets --replication-factor 1 --partitions 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic _connect-status --replication-factor 1 --partitions 1
      "

  schema-registry:
    image: confluentinc/cp-schema-registry:7.9.1
    container_name: schema-registry
    hostname: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092,PLAINTEXT_INTERNAL://kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      # SCHEMA_REGISTRY_DEBUG: "true"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/subjects || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ':9001'
    volumes:
      - minio_data:/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:9000/minio/health/live || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 minioadmin minioadmin;
        mc ls local/iot-data >/dev/null 2>&1 || mc mb local/iot-data;
        mc policy set public local/iot-data;
      "

  postgres:
    image: postgres:14
    container_name: postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: iot
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    volumes:
      - postgres_data:/var/lib/postgresql/data

  opensearch-node1:
    image: opensearchproject/opensearch:latest
    container_name: opensearch-node1
    ports:
      - 9200:9200 # REST API
      - 9600:9600 # Performance Analyzer
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1 # Nodes to look for when discovering the cluster (node1, node2, ...)
      - cluster.initial_cluster_manager_nodes=opensearch-node1 # Nodes eligibile to serve as cluster manager (node1, node2, ...)
      - bootstrap.memory_lock=true # Disable JVM heap memory swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # Set min and max JVM heap sizes
      - "DISABLE_INSTALL_DEMO_CONFIG=true" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch
      - "DISABLE_SECURITY_PLUGIN=true" # Disables Security plugin
    ulimits:
      memlock:
        soft: -1 # Set memlock to unlimited (no soft or hard limit)
        hard: -1
      nofile:
        soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536
        hard: 65536
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'curl -f http://localhost:9200/_cluster/health | grep -vq ''"status":"red"'' || exit 1',
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:latest
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601" # Expose port 5601 for web access to OpenSearch Dashboards
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch-node1:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true" # disables security dashboards plugin in OpenSearch Dashboards

  kafka-connect-plugin-download:
    image: busybox:stable
    container_name: kafka-connect-plugin-download
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./config/kafka/connect/opensearch/quickstart-opensearch.properties:/tmp/quickstart-opensearch.properties:ro
      - kafka-connect-opensearch-plugin:/usr/share/java/kafka-connect-opensearch
    command: >
      sh -c "
        wget -O /tmp/plugin.zip https://github.com/Aiven-Open/opensearch-connector-for-apache-kafka/releases/download/v3.1.1/opensearch-connector-for-apache-kafka-3.1.1.zip &&
        unzip /tmp/plugin.zip -d /usr/share/java/kafka-connect-opensearch &&
        rm /tmp/plugin.zip &&
        cp /tmp/quickstart-opensearch.properties /usr/share/java/kafka-connect-opensearch/config/quickstart-opensearch.properties &&
        echo '---- quickstart-opensearch.properties content ----' &&
        cat /usr/share/java/kafka-connect-opensearch/config/quickstart-opensearch.properties
      "

  kafka-connect-opensearch:
    image: confluentinc/cp-kafka-connect:7.9.1
    container_name: kafka-connect-opensearch
    depends_on:
      kafka:
        condition: service_healthy
      kafka-connect-plugin-download:
        condition: service_completed_successfully
      opensearch-node1:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/java/kafka-connect-opensearch
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    volumes:
      - kafka-connect-opensearch-plugin:/usr/share/java/kafka-connect-opensearch

  kafka-connect-registrator:
    image: curlimages/curl:latest
    container_name: kafka-connect-registrator
    depends_on:
      kafka-connect-opensearch:
        condition: service_healthy
    volumes:
      - ./config/kafka/connect/opensearch/opensearch-sink-config.json:/tmp/opensearch-sink-config.json:ro
      - ./config/kafka/connect/opensearch/register-opensearch-connector.sh:/tmp/register-opensearch-connector.sh:ro
    entrypoint: >
      sh -c "
        echo '‚è≥ Waiting for Kafka Connect REST to become ready...';
        for i in $(seq 1 60); do
          if curl -s http://kafka-connect-opensearch:8083/; then
            echo '‚úÖ Kafka Connect is up!';
            break;
          fi
          echo '...not ready yet';
          sleep 2;
        done

        echo 'üöÄ Registering OpenSearch sink connector...';
        curl -s -X POST -H 'Content-Type: application/json' \
          --data @/tmp/opensearch-sink-config.json \
          http://kafka-connect-opensearch:8083/connectors | tee /tmp/connector-register-response.json

        echo 'üì¶ Registration complete';
      "

  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: nessie
    depends_on:
      minio:
        condition: service_healthy
    ports:
      - "19120:19120"
    volumes:
      - ./config/nessie/application.properties:/deployments/config/application.properties:ro
    environment:
      QUARKUS_PROFILE: dev
      QUARKUS_CONFIG_LOCATIONS: file:/deployments/config/application.properties

  iot-data-generator:
    build:
      context: ./iot-data-generator
    container_name: iot-data-generator
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC: iot-raw-telemetry

  spark-stream-processor:
    build:
      context: ./spark-stream-processor
    container_name: spark-stream-processor
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      PYSPARK_PYTHON: python3
      # SPARK_LOCAL_IP: 0.0.0.0
      SPARK_BLOCKMANAGER_PORT: 7079
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_RAW_DATA_TOPIC: iot-raw-telemetry
      KAFKA_PROCESSED_DATA_TOPIC: iot-processed-telemetry
      KAFKA_STARTING_OFFSETS: latest
      SPARK_STREAMING_CHECKPOINT_LOCATION: s3a://iot-data/checkpoints/spark-stream-processor/raw-telemetry
      SPARK_STREAMING_TRIGGER_INTERVAL: 30 seconds
    volumes:
      - ./config/spark/spark-stream-processor:/opt/bitnami/spark/conf

  spark-to-iceberg-processor:
    build:
      context: ./spark-to-iceberg-processor
    container_name: spark-to-iceberg-processor
    depends_on:
      kafka:
        condition: service_healthy
      nessie:
        condition: service_started
    environment:
      PYSPARK_PYTHON: python3
      # SPARK_LOCAL_IP: 0.0.0.0
      SPARK_BLOCKMANAGER_PORT: 7079
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC: iot-processed-telemetry
      KAFKA_STARTING_OFFSETS: latest
      ICEBERG_CATALOG: iot_data
      ICEBERG_TABLE_IDENTIFIER: iot.telemetry
      SPARK_STREAMING_CHECKPOINT_LOCATION: s3a://iot-data/checkpoints/spark-to-iceberg-processor/processed-telemetry
      SPARK_STREAMING_TRIGGER_INTERVAL: 30 seconds
    volumes:
      - ./config/spark/spark-to-iceberg-processor:/opt/bitnami/spark/conf

  # trino:
  #   image: trinodb/trino:latest
  #   container_name: trino
  #   depends_on:
  #     nessie:
  #       condition: service_started
  #   ports:
  #     - "8082:8080"
  #   volumes:
  #     - ./config/trino/catalog:/etc/trino/catalog
#
# grafana:
#   image: grafana/grafana-oss:latest
#   container_name: grafana
#   depends_on:
#     trino:
#       condition: service_started
#   ports:
#     - "3000:3000"
#   environment:
#     GF_SECURITY_ADMIN_USER: admin
#     GF_SECURITY_ADMIN_PASSWORD: admin
#     GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: trino-datasource
#     GF_INSTALL_PLUGINS: trino-datasource
#   volumes:
#     # 'data/grafana' contains DEMO dashboards configurations
#     - ./data/grafana:/var/lib/grafana
#   healthcheck:
#     test: ["CMD-SHELL", "curl -f http://localhost:3000/login || exit 1"]
#     interval: 10s
#     timeout: 5s
#     retries: 5

volumes:
  kafka_data:
  zookeeper_data:
  zookeeper_log:
  minio_data:
  postgres_data:
  kafka-connect-opensearch-plugin:
  opensearch-data1:
