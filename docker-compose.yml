services:
  kafka:
    image: apache/kafka:latest
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      CLUSTER_ID: 1
      KAFKA_NODE_ID: 1
      KAFKA_KRAFT_MODE: "true" # This enables KRaft mode in Kafka.
      KAFKA_PROCESS_ROLES: controller,broker # Kafka acts as both broker and controller.
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@localhost:9093" # Defines the controller voters.
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true" # Kafka will automatically create topics if needed.
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Since weâ€™re running one broker, one replica is enough.
      KAFKA_LOG_RETENTION_HOURS: 168 # Keep logs for 7 days.
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # No delay for consumer rebalancing.
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "./opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server kafka:9092 > /dev/null 2>&1",
        ]
      interval: 10s
      timeout: 10s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ':9001'
    volumes:
      - minio_data:/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:9000/minio/health/live || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 minioadmin minioadmin;
        mc ls local/iot-data >/dev/null 2>&1 || mc mb local/iot-data;
        mc policy set public local/iot-data;
      "

  postgres:
    image: postgres:14
    container_name: postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: iot
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    volumes:
      - postgres_data:/var/lib/postgresql/data

  opensearch-node1:
    image: opensearchproject/opensearch:latest
    container_name: opensearch-node1
    ports:
      - 9200:9200 # REST API
      - 9600:9600 # Performance Analyzer
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1 # Nodes to look for when discovering the cluster (node1, node2, ...)
      - cluster.initial_cluster_manager_nodes=opensearch-node1 # Nodes eligibile to serve as cluster manager (node1, node2, ...)
      - bootstrap.memory_lock=true # Disable JVM heap memory swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # Set min and max JVM heap sizes
      - "DISABLE_INSTALL_DEMO_CONFIG=true" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch
      - "DISABLE_SECURITY_PLUGIN=true" # Disables Security plugin
    ulimits:
      memlock:
        soft: -1 # Set memlock to unlimited (no soft or hard limit)
        hard: -1
      nofile:
        soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:latest
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601" # Expose port 5601 for web access to OpenSearch Dashboards
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch-node1:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true" # disables security dashboards plugin in OpenSearch Dashboards

  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: nessie
    depends_on:
      minio:
        condition: service_healthy
    ports:
      - "19120:19120"
    volumes:
      - ./config/nessie/application.properties:/deployments/config/application.properties:ro
    environment:
      QUARKUS_PROFILE: dev
      QUARKUS_CONFIG_LOCATIONS: file:/deployments/config/application.properties

  iot-data-generator:
    build:
      context: ./iot-data-generator
    container_name: iot-data-generator
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: iot-raw-telemetry

  spark-stream-processor:
    build:
      context: ./spark-stream-processor
    container_name: spark-stream-processor
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      PYSPARK_PYTHON: python3
      # SPARK_LOCAL_IP: 0.0.0.0
      SPARK_BLOCKMANAGER_PORT: 7079
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_GROUP_ID: 1
      KAFKA_RAW_DATA_TOPIC: iot-raw-telemetry
      KAFKA_PROCESSED_DATA_TOPIC: iot-processed-telemetry
      KAFKA_STARTING_OFFSETS: latest
      SPARK_STREAMING_CHECKPOINT_LOCATION: s3a://iot-data/checkpoints/spark-stream-processor/raw-telemetry
      SPARK_STREAMING_TRIGGER_INTERVAL: 30 seconds
    volumes:
      - ./config/spark/spark-stream-processor:/opt/bitnami/spark/conf

  spark-to-iceberg-processor:
    build:
      context: ./spark-to-iceberg-processor
    container_name: spark-to-iceberg-processor
    depends_on:
      kafka:
        condition: service_healthy
      nessie:
        condition: service_started
    environment:
      PYSPARK_PYTHON: python3
      # SPARK_LOCAL_IP: 0.0.0.0
      SPARK_BLOCKMANAGER_PORT: 7079
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_GROUP_ID: 1
      KAFKA_TOPIC: iot-processed-telemetry
      KAFKA_STARTING_OFFSETS: latest
      ICEBERG_CATALOG: iot_data
      ICEBERG_TABLE_IDENTIFIER: iot.telemetry
      SPARK_STREAMING_CHECKPOINT_LOCATION: s3a://iot-data/checkpoints/spark-to-iceberg-processor/processed-telemetry
      SPARK_STREAMING_TRIGGER_INTERVAL: 30 seconds
    volumes:
      - ./config/spark/spark-to-iceberg-processor:/opt/bitnami/spark/conf

  iot-data-to-opensearch:
    build:
      context: ./iot-data-to-opensearch
    container_name: iot-data-to-opensearch
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: iot-processed-telemetry
      KAFKA_GROUP_ID: 2
      OPENSEARCH_HOST: opensearch-node1
      OPENSEARCH_PORT: 9200
      # OPENSEARCH_USERNAME: admin
      # OPENSEARCH_PASSWORD: admin
      OPENSEARCH_USE_SSL: "False"
      OPENSEARCH_VERIFY_CERTS: "False"
      OPENSEARCH_INDEX_PREFIX: iot-data
      OPENSEARCH_MAX_BATCH_SIZE: 100
      OPENSEARCH_FLUSH_INTERVAL_SECONDS: 5

  trino:
    image: trinodb/trino:latest
    container_name: trino
    depends_on:
      nessie:
        condition: service_started
    ports:
      - "8082:8080"
    volumes:
      - ./config/trino/catalog:/etc/trino/catalog

  grafana:
    image: grafana/grafana-oss:latest
    container_name: grafana
    depends_on:
      trino:
        condition: service_started
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: trino-datasource
      GF_INSTALL_PLUGINS: trino-datasource,grafana-opensearch-datasource
    volumes:
      # 'data/grafana' contains DEMO dashboards configurations
      - ./data/grafana:/var/lib/grafana
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/login || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  kafka_data:
  zookeeper_data:
  zookeeper_log:
  minio_data:
  postgres_data:
  opensearch-data1:
